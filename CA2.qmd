---
title: "APA_Assignment_2"
author: "Sam Ryder, 18317496"
format: html
editor: visual
---

## Library Installation

```{r}
#Install libraries
suppressWarnings(library(COMPoissonReg))
suppressWarnings(library(AER))
suppressWarnings(library(MASS))
suppressWarnings(library("xlsx"))
suppressWarnings(library("nlme"))
suppressWarnings(library(lme4))
suppressWarnings(library(splines))
```

## Question 1

### Part A: Poisson GLM

```{r}
#Data
data(couple)

#Fit Poisson GLM
fit.pois <- glm(UPB ~ EDUCATION + ANXIETY, data = couple, family=poisson)
summary(fit.pois)
```

Both variables significant.

Education to a significance level of 0.01 and Anxiety to a significance level of 0.001.

Coefficient Estimates show that those with a bachelor's degree are less likely to have high UPB.

Those with Anxiety are more likely to have high UPB.

### Part B: Overdispersion Test

```{r}
#Overdispersion test
dispersiontest(fit.pois, trafo = function(x) x^2)
```

Reject h0 as alpha (0.001335) \< 0.05.

This means that data is overdispersed.

To deal with overdispersion in poisson model one could use ZIP (Zero Inflated Poisson) model. ZIP model allows for modelling where large proportion of values of variable are 0 as is the case in this dataset.

### Part C: Negative binomial regression model on overdispersed data

The data has been shown to be overdispersed.

This means the use of a negative binomial regression model is suitable.

```{r}
fit.nb <- glm.nb(UPB ~ EDUCATION + ANXIETY, data = couple)
summary(fit.nb)
SE<-coef(summary(fit.nb))[,2] # standard errors
inf<-fit.nb$coef-qnorm(1-0.05/2)*SE # inferior bound
sup<-fit.nb$coef+qnorm(1-0.05/2)*SE # superior bound
round(inf,5)
round(sup,5)
```

Covariate education is non-significant as confidence interval contains the value zero.

This represents a change from the Poisson modelling as Education was significant to a 0.01 level.

### Part D: Repeated Poisson model runs

Set up data.

```{r}
x <- couple[, c('EDUCATION', 'ANXIETY')]
y <- couple$UPB
#Split data
set.seed(123)
n <- nrow(x) 
train_indices <- sample(1:n, n * 0.8)  # Randomly sample 80% of the indices
x_train <- x[train_indices, ]  # Subset the data using the sampled indices for training
x_test <- x[-train_indices, ]  # Subset the remaining data for testing
y_train <- y[train_indices]  # Subset the corresponding labels for training
y_test <- y[-train_indices]  # Subset the corresponding labels for testing
```

Train and predict using Poisson model and negative binomial model

```{r}
#Training
#Poisson model
pois.mod <- glm(y_train ~ ., data = x_train, family=poisson)
#Negative Binomial model
fit.nb <- glm.nb(y_train ~ ., data = x_train)

#Predictions
#Poisson model
poisson_pred <- predict(pois.mod, newdata = x_test, type = "response")
#Negative Binomial model
nb_pred <- predict(fit.nb, newdata = x_test, type = "response")
```

Compute the MSE

```{r}
#Compute MSE
#Poisson model
mse_poisson <- mean((y_test-poisson_pred)**2)
#Negative Binomial model
mse_binomial <- mean((y_test-nb_pred)**2)

# Print the results
cat("Mean Squared Prediction Error (MSPE) for Poisson Model:", mse_poisson, "\n")
cat("Mean Squared Prediction Error (MSPE) for Negative Binomial Model:", mse_binomial, "\n")
```

Repeat the process 100 times

```{r}
#Repeat 100 times
#Result vectors
mse_poisson_vec <- numeric(100)
mse_nb_vec <- numeric(100)


# Repeat the process 100 times
for (i in 1:100) {
  # Randomly split the data into training and test sets
  train_indices <- sample(1:nrow(x), nrow(x) * 0.5)  # 50% for training
  train <- x[train_indices, ]
  test <- x[-train_indices, ]
  y_train <- y[train_indices]
  y_test <- y[-train_indices]
  
  # Fit Poisson model to the training set
  pois.mod <- glm(y_train ~ ., data = train, family = poisson)
  
  # Fit Negative Binomial model to the training set
  fit.nb <- glm.nb(y_train ~ ., data = train)
  
  # Predictions for Poisson model on the test set
  poisson_pred <- predict(pois.mod, newdata = test, type = "response")
  
  # Predictions for Negative Binomial model on the test set
  nb_pred <- predict(fit.nb, newdata = test, type = "response")
  
  # Compute MSE for Poisson model
  mse_poisson <- mean((y_test - poisson_pred)^2)
  
  # Compute MSE for Negative Binomial model
  mse_nb <- mean((y_test - nb_pred)^2)
  
  # Store MSE values in vectors
  mse_poisson_vec[i] <- mse_poisson
  mse_nb_vec[i] <- mse_nb
}

# Compute the mean of MSE values for both models
mean_mse_poisson <- mean(mse_poisson_vec)
mean_mse_nb <- mean(mse_nb_vec)

# Print the results
cat("Mean Squared Prediction Error (MSPE) for Poisson Model:", mean_mse_poisson, "\n")
cat("Mean Squared Prediction Error (MSPE) for Negative Binomial Model:", mean_mse_nb, "\n")
```

Collect results into dataframe and plot Mean Squared Error.

```{r}
#Create a results dataframe
mse_data <- data.frame(
  Model_name = rep(c('Possion', 'Negative Binomial'), each=100),
  MSE = c(mse_poisson_vec, mse_nb_vec)
)

# Create boxplot
boxplot(MSE ~ Model_name, data = mse_data, 
        main = "Mean Squared Prediction Errors",
        xlab = "Model", ylab = "MSE",
        col = c("skyblue", "lightgreen"))
```

Boxplot shows similar performance of models across 100 iterations.

The two models have similar inter-qaurtile ranges and ranges with both having a positive outlier of over 35.

The Poisson model seems to have a marginally lower average mean squared error and so is the preferred model when it comes to prediction.

## Question 2

```{r}
#Data set up
data<-read.xlsx(file='HSAB.xlsx', sheetIndex=1, header=TRUE)
math.achieve<-data$math.achieve
school<-as.factor(data$school)
sampled_schools <- sample(unique(school), 5)
data_sampled <- subset(data, school %in% sampled_schools)
```

### Part A: Normal Regression model

```{r}
#normal regression model
fit <- lm(math.achieve ~ school, data=data_sampled)
summary(fit)
```

School is not a significant covariate.

### Part B: Fixed Effects model

```{r}
#Fixed effects model
fit2 <- lmer(math.achieve~1+(1|school))
summary(fit2)
```

Random effect for school indicates variance of 8.614 and SD of 2.935.

This shows there is significant variability between schools.

While school was not significant as a fixed factor in part a), it has become significant in the random effects model.

### Part C: Intraclass coefficient and Parameter CIs

```{r}
#Intraclass correlation coefficient
#p^ = intercept variance / (intercept variance + residual_variance)
phat = 8.614/(8.614 + 39.148)
phat
```

0.18 is total variance in outcome that can be attributed to differences between schools.

Nearly one fifth of math achievement is down to the school they go to.

```{r}
#Confidence intervals for parameters
confint(fit2, method="boot")
```

95% confidence interval for SD of random intercepts: \[2.579, 3.268\].

95% confidence interval for SD of residual: \[6.147, 6.359\].

### Part D: Predict random effects

```{r}
#Predict the random effects
random_effects <- ranef(fit2)
# Extracting school IDs
school_ids <- as.numeric(rownames(random_effects[[1]]))
# Extracting random intercepts
intercepts <- random_effects[[1]][, "(Intercept)"]
# Computing lower and upper bounds of confidence intervals
ci_lower <- intercepts - 1.96 * sqrt(VarCorr(fit2)$school[1, 1])
ci_upper <- intercepts + 1.96 * sqrt(VarCorr(fit2)$school[1, 1])
# Creating a dataframe to present the results
random_effects_df <- data.frame(School_ID = school_ids,
                                Random_Intercept = intercepts,
                                Lower_CI = ci_lower,
                                Upper_CI = ci_upper)
# Printing the results
print(random_effects_df)
```

```{r}
# Plotting
par(mar = c(5, 4, 4, 4))
barplot(random_effects_df$Random_Intercept, 
        ylim = range(c(random_effects_df$Lower_CI, random_effects_df$Upper_CI)), 
        names.arg = random_effects_df$School_ID, 
        ylab = "Random Intercept", xlab = "School ID", 
        col = "lightblue", 
        main = "Random Effects with 95% Confidence Intervals")
segments(x0 = 1:nrow(random_effects_df), 
         y0 = random_effects_df$Lower_CI, 
         x1 = 1:nrow(random_effects_df), 
         y1 = random_effects_df$Upper_CI, 
         lwd = 2, col = rgb(0, 0, 1, alpha = 0.3)) 
legend("topright", legend = "95% Confidence Interval", 
       fill = "lightblue", bty = "n") 
```

## Question 3

### Part A: Generate data sample and plot curve

```{r}
# Define the  function f(x)
f <- function(x) {
  return((cos(2 * pi * x^3))^3)
}

# 200 samples
n <- 200
# Equally spaced over the interval [0, 1]
x <- seq(0, 1, length.out = n)
# Variance of the errors
sigma_sq <- 0.04
# Generate random errors
epsilon <- rnorm(n, mean = 0, sd = sqrt(sigma_sq))
# Generate the observed y values, from given formula
y <- f(x) + epsilon
# Plot the data 
plot(x, y, col = "blue", pch = 16, xlab = "x", ylab = "y", main = "Data and True Curve")
#Add true curve, f
curve(f, add = TRUE, col = "red", lwd = 2)
#Legend
legend("topright", legend = c("Data", "True Curve"), col = c("blue", "red"), pch = c(16, NA), lwd = c(NA, 2))
```

### Part B: Fit kernel smoothing spline

```{r}
#kernel smoothing
LOOCV_error <- function(x, y, bandwidth) {
  n <- length(x)
  cv_error <- 0
  for (i in 1:n) {
    x_i <- x[-i]
    y_i <- y[-i]
    f_hat_i <- ksmooth(x_i, y_i, kernel = "normal", bandwidth = bandwidth)$y[i]
    cv_error <- cv_error + (y[i] - f_hat_i)^2
  }
  return(cv_error / n)
}

# Define a sequence of bandwidth values to evaluate
bw_values <- seq(0.01, 10, length.out = 100)
# Compute LOOCV error for each bandwidth value
cv_errors <- sapply(bw_values, function(bw) LOOCV_error(x, y, bw))
# Find the bandwidth that minimizes the LOOCV error
optimal_bw <- bw_values[which.min(cv_errors)]
# Check if there are NA values in the fitted values
if (any(is.na(ksmooth(x, y, kernel = "normal", bandwidth = optimal_bw)$y))) {
  # If NA values are present, the bandwidth might be too small. Adjust it.
  optimal_bw <- 0.1  # Or choose another reasonable value
}
# Fit a curve using kernel smoothing with the optimal bandwidth
fit_cv <- ksmooth(x, y, kernel = "normal", bandwidth = optimal_bw)

# Fit curves with small and large bandwidths for comparison
fit_small_bw <- ksmooth(x, y, kernel = "normal", bandwidth = 0.005)
fit_large_bw <- ksmooth(x, y, kernel = "normal", bandwidth = 2)

# Plot the data and the fitted curves
plot(x, y, col = "blue", pch = 16, xlab = "x", ylab = "y", main = "Data and Fitted Curves")
lines(fit_cv, col = "green", lwd = 2, lty = 1)  # Cross-validated bandwidth
lines(fit_small_bw, col = "orange", lwd = 2, lty = 2)  # Small bandwidth
lines(fit_large_bw, col = "purple", lwd = 2, lty = 3)  # Large bandwidth
legend("topright", legend = c("Data", "CV Bandwidth", "Small BW", "Large BW"), col = c("blue", "green", "orange", "purple"), pch = c(16, NA, NA, NA), lwd = c(NA, 2, 2, 2))
```

The fit in this case looks satisfactory.

```{r}
# Print out the selected bandwidth
cat("Selected bandwidth after cross-validation:", optimal_bw, "\n")
```

We can see in the graph that a bandwith of 0.005 overfits the data.

A bandwith of 2 underfits the data.

A value of 0.1, achieved through cross-validation, looks to be a good fit.

### Part C: Fit smoothing spline

```{r}
#Smoothing spline
plot(y ~ x, col = gray(0.75))
fit <- smooth.spline(x, y)
lines(fit, lty = 2)
# Get the effective degrees of freedom
df <- fit$df
print(paste("Effective Degrees of Freedom:", df))
```

A relatively high degrees of freedom and a well fit curve to the data indicate that the automatic choice for the degrees of freedom was satisfactory.

### Part D: Fit regression splines

```{r}
#Regression splines
xtilde5 <-bs(y, df = 5)
xtilde18 <-bs(y, df = 18)
# Fit regression splines with 5 and 18 degrees of freedom using xtilde5 and xtilde18
fit_5_df <- lm(y ~ xtilde5)
fit_18_df <- lm(y ~ xtilde18)
# Generate fitted values for the splines
fitted_5_df <- predict(fit_5_df)
fitted_18_df <- predict(fit_18_df)
```

Plot regression splines with all previous curve estimates

```{r}
#Plot all togteher
plot(x, y, col = "blue", pch = 16, xlab = "x", ylab = "y", main = "Data and Fitted Curves")
lines(fit_cv, col = "green", lwd = 2, lty = 1)  # Cross-validated bandwidth
lines(fit_small_bw, col = "orange", lwd = 2, lty = 2)  # Small bandwidth
lines(fit_large_bw, col = "purple", lwd = 2, lty = 3)  # Large bandwidth
lines(fit, col = "red", lwd = 2, lty = 3) # Smoothing spline
lines(x, fitted_5_df, col = "cyan", lwd = 2, lty = 3) #5df
lines(x, fitted_18_df, col = "magenta", lwd = 2, lty = 3) #18df

legend("topright", 
       legend = c("Data", "CV Bandwidth", "Small BW", "Large BW", "Smoothing Spline", "Regression (5df)", "Regression (18df)"), 
       col = c("blue", "green", "orange", "purple", "red", "cyan", "magenta"), 
       pch = c(16, NA, NA, NA, NA, NA, NA), 
       lwd = c(NA, 2, 2, 2, 2, 2, 2))

#Isolating the best fitting curves
plot(x, y, col = "blue", pch = 16, xlab = "x", ylab = "y", main = "Data and Best Fitted Curves")
lines(fit, col = "red", lwd = 2, lty = 3) # Smoothing spline
lines(fit_cv, col = "green", lwd = 2, lty = 1)  # Cross-validated bandwidth
legend("topright", 
       legend = c("Data", "CV Bandwidth", "Smoothing Spline"),
       col = c("blue", "green", "red"), 
       pch = c(16, NA, NA ), 
       lwd = c(NA, 2, 2))
```

CV bandwith and smoothing spline perform the best.
